{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoopGAN — Loop generation with StyleGAN2 and MelGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "from torchvision import utils\n",
    "from model_drum import Generator\n",
    "import sys\n",
    "sys.path.append('./melgan')\n",
    "from modules import Generator_melgan\n",
    "import os, random\n",
    "import librosa, librosa.display\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "from utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download a pre-trained model in advance\n",
    "\n",
    "``` \n",
    "$ gdown -O drumbeats1_230000.pt 1VL7kfHGMTcpniZllKZtNnN1inT9AsNzu\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image path: C:\\Users\\anecoica\\AppData\\Local\\Temp\\img_8618.png, Audio path: C:\\Users\\anecoica\\AppData\\Local\\Temp\\gem_8618.wav\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Use the Windows temporary directory\n",
    "temp_dir = os.getenv('TEMP')\n",
    "\n",
    "# Saving the generated spectrogram image\n",
    "randid = random.randint(0, 10000)\n",
    "imagepath = os.path.join(temp_dir, f'img_{randid}.png')\n",
    "filepath = os.path.join(temp_dir, f'gem_{randid}.wav')\n",
    "\n",
    "# Ensure the paths are correct\n",
    "print(f\"Image path: {imagepath}, Audio path: {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Constants - do not change! \n",
    "N_LATENT = 512\n",
    "N_MLP = 8\n",
    "SIZE_OUTPUT = 64 # size of output image\n",
    "TRUNCATION = 1 # if this is >=1, then no truncation\n",
    "VARIATION  = 0.1\n",
    "TRUNCATION_MEAN = 4096 # # of samples for getting mean latent\n",
    "SR = 44100\n",
    "\n",
    "# Number of samples (loops) in a batch\n",
    "n_samples = 4  # batch size\n",
    "\n",
    "# name of pre-trained StyleGAN2 model\n",
    "CHECKPOINT = \"./drumbeats1_230000.pt\" \n",
    "\n",
    "# mean / std of Spectrograms of training data. used for the conversion from generated spectrograms into wav files\n",
    "DATAPATH = \"./data/drumbeats_1bar/\" \n",
    "\n",
    "# name of pre-trained MelGAN model\n",
    "MELGAN_MODEL_NAME = \"best_netG.pt\"\n",
    "\n",
    "# GANSpace - # of PCA components\n",
    "N_PCA_COMPS = 6\n",
    "\n",
    "# Use \"cuda\" if you have GPUs on your machine\n",
    "device_name = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device_name)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anecoica\\AppData\\Local\\Temp\\ipykernel_11804\\625264250.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(CHECKPOINT, map_location=torch.device(device_name))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     16\u001b[0m     mean_latent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m w1, w2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# Load a StyleGAN2 model\n",
    "generator = Generator(SIZE_OUTPUT, N_LATENT, N_MLP, channel_multiplier=2).to(device_name)\n",
    "checkpoint = torch.load(CHECKPOINT, map_location=torch.device(device_name))\n",
    "\n",
    "generator.load_state_dict(checkpoint[\"g_ema\"], strict=False)\n",
    "\n",
    "# Global reference for the generator\n",
    "g_ema = generator\n",
    "g_ema.eval()  # Set the generator to evaluation mode\n",
    "mean_latent = None  # Initially set it to None\n",
    "\n",
    "if TRUNCATION < 1:\n",
    "    with torch.no_grad():\n",
    "        mean_latent = generator.mean_latent(TRUNCATION_MEAN)\n",
    "else:\n",
    "    mean_latent = None\n",
    "\n",
    "w1, w2 = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a MelGAN model\n",
    "def load_vocoder(device_name):\n",
    "    feat_dim = 80\n",
    "    mean_fp = f'{DATAPATH}/mean.mel.npy'\n",
    "    std_fp = f'{DATAPATH}/std.mel.npy'\n",
    "    v_mean = torch.from_numpy(np.load(mean_fp)).float().view(1, feat_dim, 1).to(device_name)\n",
    "    v_std = torch.from_numpy(np.load(std_fp)).float().view(1, feat_dim, 1).to(device_name)\n",
    "    \n",
    "    vocoder_config_fp = './melgan/args.yml'\n",
    "    vocoder_config = read_yaml(vocoder_config_fp)\n",
    "\n",
    "    n_mel_channels = vocoder_config.n_mel_channels\n",
    "    ngf = vocoder_config.ngf\n",
    "    n_residual_layers = vocoder_config.n_residual_layers\n",
    "\n",
    "    vocoder = Generator_melgan(n_mel_channels, ngf, n_residual_layers).to(device_name)\n",
    "    vocoder.eval()\n",
    "\n",
    "    vocoder_param_fp = os.path.join('./melgan', MELGAN_MODEL_NAME)\n",
    "    vocoder.load_state_dict(torch.load(vocoder_param_fp, map_location=torch.device(device_name)), strict=False)\n",
    "\n",
    "    return vocoder, v_mean, v_std\n",
    "\n",
    "VOCODER, V_MEAN, V_STD = load_vocoder(device_name)\n",
    "\n",
    "def vocode(sample, vocoder=VOCODER, v_mean=V_MEAN, v_std=V_STD):\n",
    "    de_norm = sample.squeeze(0) * v_std + v_mean\n",
    "    audio_output = vocoder(de_norm)\n",
    "    return audio_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GANSpace \n",
    "\n",
    "PCA on randomly generated W vectors to get meaningful style vector directions.\n",
    "\n",
    "Härkönen, Erik, Aaron Hertzmann, Jaakko Lehtinen, and Sylvain Paris. 2020. “GANSpace: Discovering Interpretable GAN Controls.” arXiv [cs.CV]. arXiv. http://arxiv.org/abs/2004.02546.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Generate random W (style vectors) with random Z \n",
    "randz = torch.randn(2000, N_LATENT, device=device_name) \n",
    "randw = generator.get_latent(randz).detach().cpu().numpy()\n",
    "\n",
    "# Find principal components using PCA \n",
    "pca = PCA(n_components=N_PCA_COMPS)\n",
    "pca.fit(randw)\n",
    "pcomponents = torch.tensor(pca.components_) # PCA components\n",
    "\n",
    "# Explained variance\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print('Explained Variance: ', explained_variance)\n",
    "\n",
    "# Scree plot\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(range(len(explained_variance)), explained_variance, alpha=0.5, align='center')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal components')\n",
    "plt.show()\n",
    "print(\"How many PCA components we need?\")\n",
    "\n",
    "# probably 6 components are enough"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation\n",
    "\n",
    "To make it easier to handle on Max/MSP, generated loops is saved as a wav file with `n_samples` channels. (default: 4ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import os\n",
    "prev_w = None\n",
    "\n",
    "# Adde PCAed W vector\n",
    "def get_w(w, coefs, scale=1.0):\n",
    "    coefs = coefs[:N_PCA_COMPS]  # Limit to N_PCA_COMPS\n",
    "    coefs_tensor = torch.tensor([coefs], device=device_name)  # Create tensor for coefficients\n",
    "    pcomponents_device = pcomponents.to(device_name)  # Ensure PCA components are on the right device\n",
    "    w_diff = pcomponents_device * coefs_tensor.T  # Multiply by PCA components\n",
    "    \n",
    "    # Add weighted PCA components to each latent vector in the batch\n",
    "    w += w_diff.sum(0)  # Sum the contributions of all PCA components\n",
    "\n",
    "    return w\n",
    "\n",
    "\n",
    "# main function\n",
    "# g_ema: stylegan generator\n",
    "# center_z: to specify the input latent z \n",
    "# trucation: GAN trucation value\n",
    "# variation: the scale of noise added to center_z (= vatiation in a batch)\n",
    "def generate(g_ema=generator, \n",
    "             w1=None,  \n",
    "             w2=None,  \n",
    "             center_z=None, \n",
    "             use_prev_w=False, \n",
    "             truncation=TRUNCATION, \n",
    "             variation=VARIATION, \n",
    "             coef=None,\n",
    "             noise=None):\n",
    "\n",
    "    global prev_w  # Maintain global state across cells\n",
    "    device = torch.device(device_name)\n",
    "    print(\"generating...\")\n",
    "    g_ema = g_ema.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        g_ema.eval()\n",
    "\n",
    "        # Style mixing logic: If w1 and w2 are provided, perform style mixing\n",
    "        if w1 is not None and w2 is not None:\n",
    "            w1, w2 = w1.to(device), w2.to(device)\n",
    "            \n",
    "            # Interpolation between w1 and w2 controlled by t\n",
    "            if t is not None:\n",
    "                mixed_w = t * w1 + (1 - t) * w2\n",
    "                latent = mixed_w.unsqueeze(1).repeat(1, g_ema.n_latent, 1)\n",
    "            \n",
    "\n",
    "        else:\n",
    "            # Generate latent vector from Z if w1 and w2 are not provided\n",
    "            if not use_prev_w or prev_w is None:\n",
    "                if center_z is None:\n",
    "                    sample_z = torch.randn(1, N_LATENT, device=device)\n",
    "                    sample_z = sample_z.repeat(n_samples, 1) + torch.randn(n_samples, N_LATENT, device=device) * variation\n",
    "                else:\n",
    "                    sample_z = center_z + torch.randn(n_samples, N_LATENT, device=device) * variation\n",
    "                latent = g_ema.get_latent(sample_z).unsqueeze(1).repeat(1, g_ema.n_latent, 1).to(device)\n",
    "                prev_w = latent.clone()\n",
    "            else:\n",
    "                if prev_w.dim() == 2:  # Ensure prev_w has two dimensions (batch_size, latent_dim)\n",
    "                    latent = prev_w.unsqueeze(1).repeat(1, g_ema.n_latent, 1).to(device)\n",
    "                else:\n",
    "                    latent = prev_w  # Use it as-is if already in the correct format\n",
    "\n",
    "        # Apply PCA coefficients if available\n",
    "        if coef:\n",
    "            latent = get_w(latent, coef)\n",
    "\n",
    "        # Apply truncation if necessary\n",
    "        if truncation < 1 and mean_latent is not None:\n",
    "            latent = truncation * (latent - mean_latent.to(device)) + mean_latent.to(device)\n",
    "\n",
    "        # Forward pass through generator\n",
    "        sample, _ = g_ema([latent], truncation=truncation, truncation_latent=mean_latent, input_is_latent=True, noise=noise)\n",
    "\n",
    "        # If the generated sample is smaller than expected, ensure at least one sample is handled\n",
    "        if sample.size(0) < n_samples:\n",
    "            sample = sample.repeat(n_samples, 1, 1, 1)\n",
    "\n",
    "        # Save and return outputs\n",
    "        temp_dir = os.getenv('TEMP')\n",
    "        randid = random.randint(0, 10000)\n",
    "        imagepath = os.path.join(temp_dir, f'img_{randid}.png')\n",
    "        filepath = os.path.join(temp_dir, f'gem_{randid}.wav')\n",
    "\n",
    "        utils.save_image(sample, imagepath, nrow=1, normalize=True, value_range=(-1, 1))\n",
    "\n",
    "        # Saving multi-channel audio file\n",
    "        channels = []\n",
    "        numpy_chs = []\n",
    "        for i in range(min(n_samples, sample.size(0))):  # Ensure we're not indexing out of bounds\n",
    "            audio_output = vocode(sample[i])\n",
    "            audio_output = audio_output.squeeze().detach().cpu().numpy()\n",
    "            numpy_chs.append(audio_output)\n",
    "            channel = AudioSegment((audio_output * np.iinfo(np.int16).max).astype(\"int16\").tobytes(),\n",
    "                                   sample_width=2,  # 16 bit\n",
    "                                   frame_rate=SR, channels=1)\n",
    "            channels.append(channel)\n",
    "\n",
    "        multich = AudioSegment.from_mono_audiosegments(*channels)\n",
    "        multich.export(filepath, format=\"wav\")\n",
    "\n",
    "        return filepath, imagepath, np.array(numpy_chs)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test\n",
    "\n",
    "Let's generate random loops with random input latent z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random generation \n",
    "_,  imagepath, audio_output = generate(variation=0.0)\n",
    "\n",
    "# display the generated spectrogram\n",
    "ipd.display(ipd.Image(filename=imagepath))\n",
    "\n",
    "\n",
    "# play the first channel\n",
    "ipd.display(ipd.Audio(audio_output[0], rate=SR))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GANSpace Inrterpolation\n",
    "interpolation of the first PCA components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline\n",
    "print(\"baseline rhythm\")\n",
    "_,  imagepath, audio_output = generate(variation=0.0)\n",
    "ipd.display(ipd.Audio(audio_output[0], rate=SR))\n",
    "print(\"------------\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# changing the 1st component\n",
    "print(\"changing the 1st PCA component\")\n",
    "#for x in np.linspace(-10, 10, 7):\n",
    "#    _, _, audio_output  = generate(center_z=None, use_prev_w=True, coef = [x,0,0,0,0,0]) \n",
    "#    print(\"1st component:\", x)\n",
    "#    ipd.display(ipd.Audio(audio_output[0], rate=SR))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stereo effect\n",
    "Small gausian noise can be added to the input latent vector z. If you play the first and second sample in the generated batch as a stereo audio file, then you'll get an interesting stereo effect. Try different numbers! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,  imagepath, audio_output = generate(variation=0.35)\n",
    "\n",
    "ipd.display(ipd.Audio(audio_output[:2], rate=SR)) # stereo \n",
    "\n",
    "ipd.display(ipd.Image(filename=imagepath))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OSC server/client for Max/MSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "audiopath, imagepath, _ = generate(center_z=None, use_prev_w=False, truncation=1.0, variation=0.1, coef=[0, 0, 0, 0, 0, 0])\n",
    "print(f\"Audio path: {audiopath}, Image path: {imagepath}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pythonosc import dispatcher\n",
    "from pythonosc import osc_server, udp_client\n",
    "import os, random\n",
    "\n",
    "# client\n",
    "client = udp_client.SimpleUDPClient('127.0.0.1', 10018)\n",
    "variation = 0.10\n",
    "truncation = 1.0\n",
    "coef = [0, 0, 0, 0, 0, 0]  # Default PCA coefficients\n",
    "\n",
    "# OSC handler to update variation\n",
    "def set_variation(unused_addr, variation_value):\n",
    "    global variation\n",
    "    variation = variation_value\n",
    "    print(f\"Variation set to: {variation}\")\n",
    "\n",
    "# OSC handler to update truncation\n",
    "def set_truncation(unused_addr, truncation_value):\n",
    "    global truncation\n",
    "    truncation = truncation_value\n",
    "    print(f\"Truncation set to: {truncation}\")\n",
    "\n",
    "# OSC handler to update PCA coefficients\n",
    "def set_coefficients(unused_addr, *coefficients):\n",
    "    global coef\n",
    "    coef = list(coefficients)[:6]  # Ensure only 6 values\n",
    "    print(f\"PCA coefficients set to: {coef}\")\n",
    "# generate randomly\n",
    "def generate_random(unused_addr, x1, x2, x3, x4, x5, x6, variation, truncation):\n",
    "    global mean_latent, g_ema\n",
    "    print(f\"Received: x1={x1}, x2={x2}, ..., variation={variation}, truncation={truncation}\")\n",
    "    \n",
    "    # Initialize mean_latent for truncation if needed\n",
    "    if truncation < 1 and mean_latent is None:\n",
    "        mean_latent = g_ema.mean_latent(TRUNCATION_MEAN)\n",
    "\n",
    "    # Set use_prev_w to False to ensure fresh random Z\n",
    "    print(\"Generating random sample...\")\n",
    "    audiopath, imagepath, _ = generate(center_z=None, \n",
    "                                       use_prev_w=False,  # Force new random latent vectors\n",
    "                                       truncation=truncation,\n",
    "                                       variation=variation, \n",
    "                                       coef=[x1, x2, x3, x4, x5, x6])  # PCA coefficients\n",
    "    \n",
    "    print(f\"Generated audio: {audiopath}, image: {imagepath}\")\n",
    "\n",
    "    client.send_message(\"/generated\", (audiopath, imagepath))\n",
    "    print(\"Message sent to Max\")\n",
    "\n",
    "\n",
    "\n",
    "# morphing\n",
    "def generate_xy(unused_addr, x1,x2,x3,x4,x5,x6, variation, truncation):\n",
    "    global mean_latent, g_ema\n",
    "    print(f\"Received: x1={x1}, x2={x2}, ..., variation={variation}, truncation={truncation}\")\n",
    "    # Initialize mean_latent for truncation if needed\n",
    "    if truncation < 1 and mean_latent is None:\n",
    "        mean_latent = g_ema.mean_latent(TRUNCATION_MEAN)\n",
    "    print(\"Generating random sample...\")     \n",
    "    audiopath, imagepath, _  = generate(center_z=None, \n",
    "                                        use_prev_w=True,\n",
    "                                        truncation=truncation,\n",
    "                                        variation=variation, \n",
    "                                        coef = [x1,x2,x3,x4,x5,x6]) # random sample\n",
    "    print(f\"Generated audio: {audiopath}, image: {imagepath}\")\n",
    "    client.send_message(\"/generated\", (audiopath, imagepath))\n",
    "    print(\"Message sent to Max\")\n",
    "\n",
    "# Style mixing function for generating mixed outputs\n",
    "def generate_mixed(unused_addr, *coefficients):\n",
    "    global mean_latent, g_ema, w1, w2\n",
    "\n",
    "    # Ensure that generator and necessary variables are initialized\n",
    "    if g_ema is None:\n",
    "        print(\"Error: g_ema is not initialized\")\n",
    "        return\n",
    "    \n",
    "    if w1 is None or w2 is None is None:\n",
    "        print(\"Error: W1, W2,  not set\")\n",
    "        return\n",
    "\n",
    "    print(f\"Using W1 ({w1.size(1)}) and W2 ({w2.size(1)})\")\n",
    "    \n",
    "    # Call the generate function with w1 and w2 for style mixing\n",
    "    audiopath, imagepath, _ = generate(w1=w1, w2=w2, truncation=truncation)\n",
    "\n",
    "    # Send the generated audio and image paths back to the client\n",
    "    client.send_message(\"/generated\", (audiopath, imagepath))\n",
    "    print(f\"Generated mixed output: {audiopath}, {imagepath}\")\n",
    "\n",
    "\n",
    "# OSC Handlers for W1 and W2\n",
    "w1_part1, w1_part2, w2_part1, w2_part2 = None, None, None, None\n",
    "\n",
    "# Handle W1 parts (split into two 256-value parts)\n",
    "def set_w1_part1(unused_addr, *w1_values):\n",
    "    global w1_part1\n",
    "    if len(w1_values) == 256:\n",
    "        w1_part1 = list(w1_values)\n",
    "        print(\"W1 part 1 set\")\n",
    "    else:\n",
    "        print(f\"Expected 256 values, received {len(w1_values)}\")\n",
    "\n",
    "def set_w1_part2(unused_addr, *w1_values):\n",
    "    global w1_part1, w1_part2, w1\n",
    "    if len(w1_values) == 256:\n",
    "        w1_part2 = list(w1_values)\n",
    "        print(\"W1 part 2 set\")\n",
    "        # Ensure both parts are received and not None\n",
    "        if w1_part1 is not None and w1_part2 is not None:\n",
    "            w1_combined = w1_part1 + w1_part2\n",
    "            w1 = torch.tensor(w1_combined, dtype=torch.float32).view(1, 512).to(device_name)\n",
    "            print(\"W1 fully set\")\n",
    "            # Reset parts after combining\n",
    "            w1_part1 = None\n",
    "            w1_part2 = None\n",
    "    else:\n",
    "        print(f\"Expected 256 values, received {len(w1_values)}\")\n",
    "\n",
    "# Handle W2 parts\n",
    "def set_w2_part1(unused_addr, *w2_values):\n",
    "    global w2_part1\n",
    "    if len(w2_values) == 256:\n",
    "        w2_part1 = list(w2_values)\n",
    "        print(\"W2 part 1 set\")\n",
    "    else:\n",
    "        print(f\"Expected 256 values, received {len(w2_values)}\")\n",
    "\n",
    "def set_w2_part2(unused_addr, *w2_values):\n",
    "    global w2_part1, w2_part2, w2\n",
    "    if len(w2_values) == 256:\n",
    "        w2_part2 = list(w2_values)\n",
    "        print(\"W2 part 2 set\")\n",
    "        # Ensure both parts are received and not None\n",
    "        if w2_part1 is not None and w2_part2 is not None:\n",
    "            w2_combined = w2_part1 + w2_part2\n",
    "            w2 = torch.tensor(w2_combined, dtype=torch.float32).view(1, 512).to(device_name)\n",
    "            print(\"W2 fully set\")\n",
    "            # Reset parts after combining\n",
    "            w2_part1 = None\n",
    "            w2_part2 = None\n",
    "    else:\n",
    "        print(f\"Expected 256 values, received {len(w2_values)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def set_interpolation_t(unused_addr, t_value):\n",
    "    global t\n",
    "    t = t_value\n",
    "    print(f\"Interpolation factor t set to: {t}\")\n",
    "\n",
    "# OSC Dispatcher Setup\n",
    "dispatcher = dispatcher.Dispatcher()\n",
    "dispatcher.map(\"/set_variation\", set_variation)\n",
    "dispatcher.map(\"/set_truncation\", set_truncation)\n",
    "dispatcher.map(\"/set_coefficients\", set_coefficients)\n",
    "dispatcher.map(\"/generate_random\", generate_random)\n",
    "dispatcher.map(\"/generate_xy\", generate_xy)\n",
    "dispatcher.map(\"/set_w1_part1\", set_w1_part1)\n",
    "dispatcher.map(\"/set_w1_part2\", set_w1_part2)\n",
    "dispatcher.map(\"/set_w2_part1\", set_w2_part1)\n",
    "dispatcher.map(\"/set_w2_part2\", set_w2_part2)\n",
    "\n",
    "dispatcher.map(\"/generate_mixed\", generate_mixed)\n",
    "dispatcher.map(\"/set_interpolation_t\", set_interpolation_t)\n",
    "\n",
    "\n",
    "# OSC Server Setup\n",
    "server = osc_server.ThreadingOSCUDPServer(('localhost', 10015), dispatcher)\n",
    "print(f\"Serving on {server.server_address}\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        server.handle_request()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Server stopped.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b5cca8b1728c569639938642ad291f1ea926bc04e4007ecfce4c4d99545e9079"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
